{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From ML with R\n",
    "\"The goal of evaluating a classification model is to have a better understanding of how its performance with extrapolate to future cases. SInce it is usually not feasible to test a still unprove model in a live, environment, we typically simulate fugure conditions by asking the model to classify a dataset made of cases that resimble what it woll be asked to do in the future. By observing the learner's responses to this examination, we can learn abou tits strenghts ans weaknesses\"\n",
    "\n",
    "We accomplish this by comparing the model's predicted class values to the actual class values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial draws code and content from Kevin Markham's General Assembly data science course https://github.com/justmarkham/DAT8, maykulkarni's machine learning notebooks repo [https://github.com/maykulkarni] \n",
    "\n",
    "Content is also drawn from An Introduction to Statistical Learning, Applied Predictive Modeling, and Machine Learning with R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "The simplest measure of a classifier's performance is overall accuracy. In overall accuracy, we divide the number of predictions the classifier got right by the total number of predictions made\n",
    "<br/><br/>\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{# Correct Predictions}}{\\text{Total # of Predictions}}$$\n",
    "\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that we are trying to build a model to filter out spam email. The following data contains the text of email messages along with their actual type (i.e., label). \"Ham\" messages constitute real email messages whereas \"spam\" messages are, well, spam.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type\n",
       "0   ham\n",
       "1   ham\n",
       "2   ham\n",
       "3  spam\n",
       "4  spam"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data from https://github.com/PacktPublishing/Machine-Learning-with-R-Third-Edition\n",
    "spam_ham = pd.read_csv('sms_spam.csv')\n",
    "spam_ham = spam_ham[['type']]\n",
    "spam_ham.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a quick look at the type column, we can see that most messages are ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>outcome_counts</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>4812</td>\n",
       "      <td>0.865623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>747</td>\n",
       "      <td>0.134377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  outcome_counts  percentage\n",
       "0   ham            4812    0.865623\n",
       "1  spam             747    0.134377"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_df = spam_ham [['type']].groupby(['type']).size().reset_index(name='outcome_counts')\n",
    "\n",
    "counts_df['percentage'] = counts_df['outcome_counts'] / counts_df['outcome_counts'].sum()\n",
    "\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5559"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t= np.zeros(spam_ham.size)\n",
    "t.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this data, we can build a very simple classifier, which just predicts the majority class. In other words, the classifier will always predict \"ham.\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of majority class model is: 0.87\n"
     ]
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "#dummy_clf = DummyClassifier(strategy=\"stratified\")\n",
    "#Note, by definition, the features are ignored in this model\n",
    "dummy_clf.fit(np.zeros(spam_ham.size), spam_ham[['type']])\n",
    "#dummy_clf.fit(spam_ham[['text']], spam_ham[['type']])\n",
    "dummy_predictions = dummy_clf.predict(np.zeros(spam_ham.size))\n",
    "\n",
    "accuracy = round(accuracy_score(spam_ham[['type']], dummy_predictions),2)\n",
    "print(f'Accuracy of majority class model is: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging by overall accuracy alone, we might conclude that the dummy classifier is doing a decent job. But of course, this is is not a good model since it never predicts \"spam,\" which is what we are actually trying to get the model to do correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A confusion matrix is used for classification models. It allows us to break down our results in terms of true positives, true negatives, false positives, and false negatives. This gives us a better sense of the kinds of errors our model is making\n",
    "<br/><br/>\n",
    "\n",
    "|            | Predicted true | Predicted false |\n",
    "|------------|----------------|-----------------|\n",
    "|Actual true | True Positive  | False Negative  |\n",
    "|Actual false| False Positive | True Negative   |\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "Accordingly, we can reframe our formula for accuracy as\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "<br/><br/>\n",
    "\n",
    "We can create a confusion matrix from our majority class model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_spam</th>\n",
       "      <th>predicted_ham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual_spam</th>\n",
       "      <td>0</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_ham</th>\n",
       "      <td>0</td>\n",
       "      <td>4812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             predicted_spam  predicted_ham\n",
       "actual_spam               0            747\n",
       "actual_ham                0           4812"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(spam_ham[['type']], dummy_predictions)\n",
    "\n",
    "#need to flip so spam is the positive case\n",
    "cm = np.flip(cm)\n",
    "\n",
    "\n",
    "cm_df = pd.DataFrame(cm, \n",
    "               columns=['predicted_spam', 'predicted_ham'], \n",
    "               index = ['actual_spam', 'actual_ham'])\n",
    "\n",
    "\n",
    "\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we'll consider \"spam\" the positive case since that is what we are trying to detect. So, cases where the model predicted \"ham\" but the message was actually \"spam\" are false negatives. We have 747 of these cases.\n",
    "\n",
    "Because the dummy classifier always predicts \"ham,\" there are no cases where the model predicted \"spam\" and the actual outcome was \"ham;\" however, if it had, those would constitute false positives.\n",
    "\n",
    "Even though our model performed well in terms of overall accuracy, we can see from the confusion matrix that it performed abysmally in terms of false negatives. In machine learning, overall accuracy provides a poor measure of a model's performance when we have a \"class imbalance,\" meaning one label occurs much more frequently than the other(s). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often with machine learning, we neither want a model that is too conservative in predicting the positive class nor too aggressive. Two metrics that help us assess this are precision and recall\n",
    "\n",
    "### Precision \n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "### Recall \n",
    "\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "In plain language, prediction captures the proportion of positive examples that are actually positive. That is, when the model predicts the positive label, how often is it correct? A precise model will only predict the positive class when the example is very likely to be positive.\n",
    "\n",
    "Recall is a measure of how complete the positive predictions are. A model that has high recall will capture a large proportion of the actual positive examples. \n",
    "\n",
    "We will calculate precision and recall for our dummy classifier, but first we need to do some minor modifications on our predictions to make this possible. If we attempt to calculate these metrics from just the majority class predictions, we'll get into some divide by zero issues for precision since we have no spam predictions. \n",
    "\n",
    "I'll demonstrate this below, just to show you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is nan\n",
      "Recall is 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-3793712a4989>:6: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  print (f'Precision is {TP/(TP + FP)}')\n"
     ]
    }
   ],
   "source": [
    "TP = cm_df.loc['actual_spam', 'predicted_spam']\n",
    "TN = cm_df.loc['actual_ham', 'predicted_ham']\n",
    "FP = cm_df.loc['actual_ham', 'predicted_spam']\n",
    "FN = cm_df.loc['actual_spam', 'predicted_ham']\n",
    "\n",
    "print (f'Precision is {TP/(TP + FP)}')\n",
    "print (f'Recall is {TP/(TP + FN)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. For illustrative purposes, we'll add a few additional observations to our dataset that will allow us to calculate these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_spam</th>\n",
       "      <th>predicted_ham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual_spam</th>\n",
       "      <td>6</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_ham</th>\n",
       "      <td>2</td>\n",
       "      <td>4812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             predicted_spam  predicted_ham\n",
       "actual_spam               6            747\n",
       "actual_ham                2           4812"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = pd.DataFrame(['spam', 'spam', 'spam', 'spam', \n",
    "                         'spam', 'spam', 'ham', 'ham'], columns=['type'])\n",
    "\n",
    "\n",
    "new_predictions = np.array(['spam', 'spam', 'spam', 'spam', \n",
    "                                'spam', 'spam', 'spam', 'spam'])\n",
    "\n",
    "new_spam_ham = pd.concat([spam_ham, new_data], ignore_index=True)\n",
    "new_dummy_predictions = np.append(dummy_predictions, new_predictions)\n",
    "\n",
    "new_cm = confusion_matrix(new_spam_ham[['type']], new_dummy_predictions)\n",
    "\n",
    "#need to flip so spam is the positive case\n",
    "new_cm = np.flip(new_cm)\n",
    "\n",
    "\n",
    "new_cm_df = pd.DataFrame(new_cm, \n",
    "               columns=['predicted_spam', 'predicted_ham'], \n",
    "               index = ['actual_spam', 'actual_ham'])\n",
    "\n",
    "\n",
    "\n",
    "new_cm_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new confusion matrix reveals that we have 6 true positives, 4812 true negatives, 2 false positives, and 747 false negatives. With these, we can now calculate precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is 0.75\n",
      "Recall is 0.00796812749003984\n"
     ]
    }
   ],
   "source": [
    "TP = new_cm_df.loc['actual_spam', 'predicted_spam']\n",
    "TN = new_cm_df.loc['actual_ham', 'predicted_ham']\n",
    "FP = new_cm_df.loc['actual_ham', 'predicted_spam']\n",
    "FN = new_cm_df.loc['actual_spam', 'predicted_ham']\n",
    "\n",
    "dc_precision = TP/(TP + FP)\n",
    "dc_recall = TP/(TP + FN)\n",
    "\n",
    "print (f'Precision is {dc_precision}')\n",
    "print (f'Recall is {dc_recall}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is relatively better than recall. Why is this? We have designed our dummy classifier so that it almost never predicts \"spam.\" The data we added contained only 8 predictions of \"spam,\" 6 of which were correct (6/8 = 0.75). So, when our dummy model does predict the positive class (i.e., \"spam\"), it does a pretty decent job. It's a reasonably precise model.\n",
    "\n",
    "What our model does not do well is cover all the actual instances of spam in the dataset. This is captured in our recall measure, which is very low. \n",
    "\n",
    "Precision and recall can tell us a lot about how our model is performing, but it's nice to have a single summary statistic as well. The F1 Score or F-measure, which combines precision and recall using the harmonic mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "\n",
    "F1 score is the harmonic mean of precision and recall. \n",
    "\n",
    "\n",
    "$$\\text{F}_1 = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01576872536136662\n"
     ]
    }
   ],
   "source": [
    "dc_F1 = 2* ((dc_precision*dc_recall)/(dc_precision + dc_recall))\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, we can also calculate all these metrics using the metrics functionality of scikit learn. We just need to convert our classes to binary first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01576872536136662"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_spam_ham['pos_case'] = np.where(new_spam_ham['type']== 'spam', 1, 0)\n",
    "new_dummy_predictions_binary = np.where(new_dummy_predictions == \"spam\", 1,0)\n",
    "f1_score(new_spam_ham[['pos_case']], new_dummy_predictions_binary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other metrics for evaluating classification models. Techniques that may be a particular help for visualizing results are precision recall curves or receiver operating characteristic (ROC) curves. You can learn more about these techniques here [INSERT LINK]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Metrics\n",
    "\n",
    "So far we have only discussed metrics for evaluating classification models, but of course, there are metrics for evaluating regression models as well. Several metrics that are commonly used include mean absolute error, mean squared error, and root mean squared error. Here, y_{i} $y_i$$ is the actual observed value for observation i and $$\\hat{y}_i$$ is the predicted value for observation i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n",
    " \n",
    "**Mean Squared Error** (MSE) is the mean of the squared errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n",
    "\n",
    "$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n",
    "\n",
    "example true and predicted response values\n",
    "true = [10, 7, 5, 5]\n",
    "pred = [8, 6, 5, 10]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate these metrics by hand!\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "print 'MAE:', metrics.mean_absolute_error(true, pred)\n",
    "print 'MSE:', metrics.mean_squared_error(true, pred)\n",
    "print 'RMSE:', np.sqrt(metrics.mean_squared_error(true, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will need some predicted probabilities for this to work\n",
    "#calculate precision and recall\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_score)\n",
    "\n",
    "#create precision recall curve\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(recall, precision, color='purple')\n",
    "\n",
    "#add axis labels to plot\n",
    "ax.set_title('Precision-Recall Curve')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_xlabel('Recall')\n",
    "\n",
    "#display plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "### Precision (Positive Predicted Value) \n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "Intuitively, what precision states is out of the number of times your model predicts true, how many times is it correct? This metric penalizes heavily for False Positives. This metric should be considered when its OK to have some false negatives but not false positives. Imagine if your model is predicting the conclusion of a jurisdiction. Its OK to leave a criminal free, rather than punishing an innocent one. \n",
    "\n",
    "### Recall (Sensitivity) \n",
    "\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "Intuitively, what recall states is out of the times the output is true, how many times are you correct? This metric penalizes heavily for False Negatives. This metric should be considered when its OK to have some false positives but not false negatives.\n",
    "\n",
    "\n",
    "### F1 Score\n",
    "\n",
    "F1 score is the harmonic mean of precision and recall. \n",
    "\n",
    "\n",
    "$$\\text{F}_1 = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X, y = make_classification()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "clf = SVC().fit(X_train, y_train)\n",
    "confusion_matrix(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|            | predicted true | predicted false |\n",
    "|------------|----------------|-----------------|\n",
    "|actual true |        10      |        0        |\n",
    "|actual false|         1      |        9        |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
