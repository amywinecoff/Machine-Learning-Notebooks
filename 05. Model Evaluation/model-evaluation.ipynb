{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From ML with R\n",
    "\"The goal of evaluating a classification model is to have a better understanding of how its performance with extrapolate to future cases. SInce it is usually not feasible to test a still unprove model in a live, environment, we typically simulate fugure conditions by asking the model to classify a dataset made of cases that resimble what it woll be asked to do in the future. By observing the learner's responses to this examination, we can learn abou tits strenghts ans weaknesses\"\n",
    "\n",
    "We accomplish this by comparing the model's predicted class values to the actual class values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "The simplest measure of a classifier's performance is overall accuracy. In overall accuracy, we divide the number of predictions the classifier got right by the total number of predictions made\n",
    "<br/><br/>\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{# Correct Predictions}}{\\text{Total # of Predictions}}$$\n",
    "\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that we are trying to build a model to filter out spam email. The following data contains the text of email messages along with their actual type (i.e., label). \"Ham\" messages constitute real email messages whereas \"spam\" messages are, well, spam.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hope you are having a good week. Just checking in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>K..give back my thanks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Am also doing in cbe only. But have to pay.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>complimentary 4 STAR Ibiza Holiday or £10,000 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>okmail: Dear Dave this is your final notice to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               text\n",
       "0   ham  Hope you are having a good week. Just checking in\n",
       "1   ham                            K..give back my thanks.\n",
       "2   ham        Am also doing in cbe only. But have to pay.\n",
       "3  spam  complimentary 4 STAR Ibiza Holiday or £10,000 ...\n",
       "4  spam  okmail: Dear Dave this is your final notice to..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data from https://github.com/PacktPublishing/Machine-Learning-with-R-Third-Edition\n",
    "spam_ham = pd.read_csv('sms_spam.csv')\n",
    "spam_ham.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a quick look at the type column, we can see that most messages are ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>outcome_counts</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>4812</td>\n",
       "      <td>0.865623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>747</td>\n",
       "      <td>0.134377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  outcome_counts  percentage\n",
       "0   ham            4812    0.865623\n",
       "1  spam             747    0.134377"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_df = spam_ham [['type']].groupby(['type']).size().reset_index(name='outcome_counts')\n",
    "\n",
    "counts_df['percentage'] = counts_df['outcome_counts'] / counts_df['outcome_counts'].sum()\n",
    "\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this data, we can build a very simple classifier, which just predicts each class with the frequency with which it occurs in the labels. In our dataset about 87% of examples are ham, so the dummy classifier will predict \"ham\" about 87% of the time.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of majority class model is: 0.77\n"
     ]
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf = DummyClassifier(strategy=\"stratified\")\n",
    "#Note, by definition, the features are ignored in this model\n",
    "dummy_clf.fit(spam_ham[['text']], spam_ham[['type']])\n",
    "dummy_predictions = dummy_clf.predict(spam_ham[['text']])\n",
    "\n",
    "# #Confirm that all predictions are indeed the majority class\n",
    "# for e in np.unique(dummy_predictions, return_counts=True):\n",
    "#     print (e[0])\n",
    "    \n",
    "accuracy = round(accuracy_score(spam_ham[['type']], dummy_predictions),2)\n",
    "print(f'Accuracy of majority class model is: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging by overall accuracy alone, we might conclude that the dummy classifier is doing a decent job. But of course, this is is not a good model since all that it is doing is probabalistically assigning a label based on class label prevalence. We need to dive a bit deeper to assess how well it is doing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A confusion matrix is used for classification models. It allows us to break down our results in terms of true positives, true negatives, false positives, and false negatives. This gives us a better sense of the kinds of errors our model is making\n",
    "<br/><br/>\n",
    "\n",
    "|            | Predicted true | Predicted false |\n",
    "|------------|----------------|-----------------|\n",
    "|Actual true | True Positive  | False Negative  |\n",
    "|Actual false| False Positive | True Negative   |\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "Accordingly, we can reframe our formula for accuracy as\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "<br/><br/>\n",
    "\n",
    "We can create a confusion matrix from our majority class model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_spam</th>\n",
       "      <th>predicted_ham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual_spam</th>\n",
       "      <td>100</td>\n",
       "      <td>647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_ham</th>\n",
       "      <td>656</td>\n",
       "      <td>4156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             predicted_spam  predicted_ham\n",
       "actual_spam             100            647\n",
       "actual_ham              656           4156"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(spam_ham[['type']], dummy_predictions)\n",
    "\n",
    "#need to flip so spam is the positive case\n",
    "cm = np.flip(cm)\n",
    "\n",
    "\n",
    "cm_df = pd.DataFrame(cm, \n",
    "               columns=['predicted_spam', 'predicted_ham'], \n",
    "               index = ['actual_spam', 'actual_ham'])\n",
    "\n",
    "\n",
    "\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we'll consider \"spam\" the positive case, and \"ham\" the negative case. So, cases where the model predicted \"ham\" but the message was actually \"spam\" are false negatives. Cases where our model predicted \"spam\" but the message was actually \"ham\" are false positives. \n",
    "\n",
    "Often with machine learning, we neither want a model that is too conservative in predicting the positive class nor too aggressive. Two metrics that help us assess this are sensitivity and specificity\n",
    "\n",
    "### Sensitivity (True Positive Rate)\n",
    "\n",
    "$$\\text{Sensitivity} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "### Specificity (True Negative Rate)\n",
    "\n",
    "$$\\text{Specificity} = \\frac{TN}{TN + FN}$$\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "We can calculate this for our dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity is 0.13227513227513227\n",
      "Specificity is 0.8652925255048928\n"
     ]
    }
   ],
   "source": [
    "TP = cm_df.loc['actual_spam', 'predicted_spam']\n",
    "TN = cm_df.loc['actual_ham', 'predicted_ham']\n",
    "FP = cm_df.loc['actual_ham', 'predicted_spam']\n",
    "FN = cm_df.loc['actual_spam', 'predicted_ham']\n",
    "\n",
    "print (f'Sensitivity is {round(TP/(TP + FP), 2)}')\n",
    "print (f'Specificity is {round(TN/(TN + FN),2)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7656053246986868"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(TP + TN) / (TP + TN + FP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity looks at all the examples where the classifier assigned the \"spam\" label. \n",
    "Specificity looks at all the examples where the classifier assigned the \"ham\" label. In actuality, about 87% of the messages were \"ham,\" so we would expect roughly that percentage of the classifier's \"ham\" guesses to be correct. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "### Precision (Positive Predicted Value) \n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "Intuitively, what precision states is out of the number of times your model predicts true, how many times is it correct? This metric penalizes heavily for False Positives. This metric should be considered when its OK to have some false negatives but not false positives. Imagine if your model is predicting the conclusion of a jurisdiction. Its OK to leave a criminal free, rather than punishing an innocent one. \n",
    "\n",
    "### Recall (Sensitivity) \n",
    "\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "Intuitively, what recall states is out of the times the output is true, how many times are you correct? This metric penalizes heavily for False Negatives. This metric should be considered when its OK to have some false positives but not false negatives.\n",
    "\n",
    "\n",
    "### F1 Score\n",
    "\n",
    "F1 score is the harmonic mean of precision and recall. \n",
    "\n",
    "\n",
    "$$\\text{F}_1 = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  0],\n",
       "       [ 1,  9]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X, y = make_classification()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "clf = SVC().fit(X_train, y_train)\n",
    "confusion_matrix(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|            | predicted true | predicted false |\n",
    "|------------|----------------|-----------------|\n",
    "|actual true |        10      |        0        |\n",
    "|actual false|         1      |        9        |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
