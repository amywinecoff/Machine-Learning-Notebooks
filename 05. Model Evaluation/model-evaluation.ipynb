{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From ML with R\n",
    "\"The goal of evaluating a classification model is to have a better understanding of how its performance with extrapolate to future cases. SInce it is usually not feasible to test a still unprove model in a live, environment, we typically simulate fugure conditions by asking the model to classify a dataset made of cases that resimble what it woll be asked to do in the future. By observing the learner's responses to this examination, we can learn abou tits strenghts ans weaknesses\"\n",
    "\n",
    "We accomplish this by comparing the model's predicted class values to the actual class values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "The simplest measure of a classifier's performance is overall accuracy. In overall accuracy, we divide the number of predictions the classifier got right by the total number of predictions made\n",
    "<br/><br/>\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{# Correct Predictions}}{\\text{Total # of Predictions}}$$\n",
    "\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that we are trying to build a model to filter out spam email. The following data contains the text of email messages along with their actual type (i.e., label). \"Ham\" messages constitute real email messages whereas \"spam\" messages are, well, spam.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type\n",
       "0   ham\n",
       "1   ham\n",
       "2   ham\n",
       "3  spam\n",
       "4  spam"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data from https://github.com/PacktPublishing/Machine-Learning-with-R-Third-Edition\n",
    "spam_ham = pd.read_csv('sms_spam.csv')\n",
    "spam_ham = spam_ham[['type']]\n",
    "spam_ham.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a quick look at the type column, we can see that most messages are ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>outcome_counts</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>4812</td>\n",
       "      <td>0.865623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>747</td>\n",
       "      <td>0.134377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  outcome_counts  percentage\n",
       "0   ham            4812    0.865623\n",
       "1  spam             747    0.134377"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_df = spam_ham [['type']].groupby(['type']).size().reset_index(name='outcome_counts')\n",
    "\n",
    "counts_df['percentage'] = counts_df['outcome_counts'] / counts_df['outcome_counts'].sum()\n",
    "\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5559"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t= np.zeros(spam_ham.size)\n",
    "t.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this data, we can build a very simple classifier, which just predicts the majority class. In other words, the classifier will always predict \"ham.\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of majority class model is: 0.87\n"
     ]
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "#dummy_clf = DummyClassifier(strategy=\"stratified\")\n",
    "#Note, by definition, the features are ignored in this model\n",
    "dummy_clf.fit(np.zeros(spam_ham.size), spam_ham[['type']])\n",
    "#dummy_clf.fit(spam_ham[['text']], spam_ham[['type']])\n",
    "dummy_predictions = dummy_clf.predict(np.zeros(spam_ham.size))\n",
    "\n",
    "accuracy = round(accuracy_score(spam_ham[['type']], dummy_predictions),2)\n",
    "print(f'Accuracy of majority class model is: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging by overall accuracy alone, we might conclude that the dummy classifier is doing a decent job. But of course, this is is not a good model since it never predicts \"spam,\" which is what we are actually trying to get the model to do correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A confusion matrix is used for classification models. It allows us to break down our results in terms of true positives, true negatives, false positives, and false negatives. This gives us a better sense of the kinds of errors our model is making\n",
    "<br/><br/>\n",
    "\n",
    "|            | Predicted true | Predicted false |\n",
    "|------------|----------------|-----------------|\n",
    "|Actual true | True Positive  | False Negative  |\n",
    "|Actual false| False Positive | True Negative   |\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "Accordingly, we can reframe our formula for accuracy as\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "<br/><br/>\n",
    "\n",
    "We can create a confusion matrix from our majority class model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_spam</th>\n",
       "      <th>predicted_ham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual_spam</th>\n",
       "      <td>0</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_ham</th>\n",
       "      <td>0</td>\n",
       "      <td>4812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             predicted_spam  predicted_ham\n",
       "actual_spam               0            747\n",
       "actual_ham                0           4812"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(spam_ham[['type']], dummy_predictions)\n",
    "\n",
    "#need to flip so spam is the positive case\n",
    "cm = np.flip(cm)\n",
    "\n",
    "\n",
    "cm_df = pd.DataFrame(cm, \n",
    "               columns=['predicted_spam', 'predicted_ham'], \n",
    "               index = ['actual_spam', 'actual_ham'])\n",
    "\n",
    "\n",
    "\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we'll consider \"spam\" the positive case since that is what we are trying to detect. So, cases where the model predicted \"ham\" but the message was actually \"spam\" are false negatives. Cases where our model predicted \"spam\" but the message was actually \"ham\" are false positives. \n",
    "\n",
    "Even though our model performed well in terms of overall accuracy, we can see from the confusion matrix that it performed abysmally in terms of false negatives. In machine learning, overall accuracy provides a poor measure of a model's performance when we have a \"class imbalance,\" meaning one label occurs much more frequently than the other(s). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often with machine learning, we neither want a model that is too conservative in predicting the positive class nor too aggressive. Two metrics that help us assess this are precision and recall\n",
    "\n",
    "### Precision \n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "### Recall \n",
    "\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "In plain language, prediction captures the proportion of positive examples that are actually positive. That is, when the model predicts the positive label, how often is it correct? A precise model will only predict the positive class when the example is very likely to be positive.\n",
    "\n",
    "Recall is a measure of how complete the positive predictions are. A model that has high recall will capture a large proportion of the actual positive examples. \n",
    "\n",
    "We will calculate precision and recall for our dummy classifier, but first we need to do some minor modifications on our predictions to make this possible. Since our dummy classifier produces no instances of the \"spam\" label, we will get into some divide by 0 issues if we tried to compute these metrics as is. To avoid this, we'll add a few cases of correct and incorrect \"spam\" predictions to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_spam</th>\n",
       "      <th>predicted_ham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual_spam</th>\n",
       "      <td>6</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_ham</th>\n",
       "      <td>2</td>\n",
       "      <td>4812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             predicted_spam  predicted_ham\n",
       "actual_spam               6            747\n",
       "actual_ham                2           4812"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = pd.DataFrame(['spam', 'spam', 'spam', 'spam', \n",
    "                         'spam', 'spam', 'ham', 'ham'], columns=['type'])\n",
    "\n",
    "\n",
    "new_predictions = np.array(['spam', 'spam', 'spam', 'spam', \n",
    "                                'spam', 'spam', 'spam', 'spam'])\n",
    "\n",
    "new_spam_ham = pd.concat([spam_ham, new_data], ignore_index=True)\n",
    "new_dummy_predictions = np.append(dummy_predictions, new_predictions)\n",
    "\n",
    "new_cm = confusion_matrix(new_spam_ham[['type']], new_dummy_predictions)\n",
    "\n",
    "#need to flip so spam is the positive case\n",
    "new_cm = np.flip(new_cm)\n",
    "\n",
    "\n",
    "new_cm_df = pd.DataFrame(new_cm, \n",
    "               columns=['predicted_spam', 'predicted_ham'], \n",
    "               index = ['actual_spam', 'actual_ham'])\n",
    "\n",
    "\n",
    "\n",
    "new_cm_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is 0.75\n",
      "Recall is 0.00796812749003984\n"
     ]
    }
   ],
   "source": [
    "TP = new_cm_df.loc['actual_spam', 'predicted_spam']\n",
    "TN = new_cm_df.loc['actual_ham', 'predicted_ham']\n",
    "FP = new_cm_df.loc['actual_ham', 'predicted_spam']\n",
    "FN = new_cm_df.loc['actual_spam', 'predicted_ham']\n",
    "\n",
    "print (f'Precision is {TP/(TP + FP)}')\n",
    "print (f'Recall is {TP/(TP + FN)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5/8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is relatively better than recall. Why is this? We have designed our dummy classifier so that it almost never predicts \"spam.\" The data we added contained only 8 predictions of \"spam,\" 6 of which were correct (6/8 = 0.75). So, when our dummy model does predict the positive class (i.e., \"spam\"), it does a pretty decent job. It's a reasonably precise model.\n",
    "\n",
    "What our model does not do well is cover \n",
    "\n",
    "\n",
    "Why is this? This is driven by our data having a \"class imbalance.\" That is, one class is more prevalent than the other. In our data, only about 13% of the examples are actually spam, but our model predicts \"spam\" 50% of the time. So, the model will overpredict \"spam,\" making it not very precise. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity looks at all the examples where the classifier assigned the \"spam\" label. \n",
    "Specificity looks at all the examples where the classifier assigned the \"ham\" label. In actuality, about 87% of the messages were \"ham,\" so we would expect roughly that percentage of the classifier's \"ham\" guesses to be correct. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "### Precision (Positive Predicted Value) \n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "Intuitively, what precision states is out of the number of times your model predicts true, how many times is it correct? This metric penalizes heavily for False Positives. This metric should be considered when its OK to have some false negatives but not false positives. Imagine if your model is predicting the conclusion of a jurisdiction. Its OK to leave a criminal free, rather than punishing an innocent one. \n",
    "\n",
    "### Recall (Sensitivity) \n",
    "\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "Intuitively, what recall states is out of the times the output is true, how many times are you correct? This metric penalizes heavily for False Negatives. This metric should be considered when its OK to have some false positives but not false negatives.\n",
    "\n",
    "\n",
    "### F1 Score\n",
    "\n",
    "F1 score is the harmonic mean of precision and recall. \n",
    "\n",
    "\n",
    "$$\\text{F}_1 = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  0],\n",
       "       [ 1,  9]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X, y = make_classification()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "clf = SVC().fit(X_train, y_train)\n",
    "confusion_matrix(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|            | predicted true | predicted false |\n",
    "|------------|----------------|-----------------|\n",
    "|actual true |        10      |        0        |\n",
    "|actual false|         1      |        9        |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
